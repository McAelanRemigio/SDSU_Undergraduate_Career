{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6tj_KB-BscE"
      },
      "source": [
        "# Welcome to the Handshake Data Intern Interview Notebook\n",
        "This notebook will be used during your virtual \"onsite\" interview. The pre-made cells below will set everything up for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD-EtshSDFiP"
      },
      "source": [
        "## Context\n",
        "\n",
        "This notebook contains a Python scripting exercise completed during a technical interview process while I was enrolled in LING 572. I include it here because it closely reflects the type of applied problem-solving and scripting emphasized in the course.\n",
        "\n",
        "The solution prioritizes clarity and correctness in core Python rather than distributed or Spark-based approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQb5JlMWDLSE"
      },
      "source": [
        "## Google Colab Resources\n",
        "Here are some example notebooks that demonstrate some capabilities in Google Colab:\n",
        "* Basic features: https://colab.research.google.com/notebooks/basic_features_overview.ipynb\n",
        "* Charting: https://colab.research.google.com/notebooks/charts.ipynb\n",
        "* Loading/saving data: https://colab.research.google.com/notebooks/io.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjGcmlh7S7If"
      },
      "source": [
        "# Import Pandas and Load Some Data\n",
        "Run these cells first. They import pandas and then load the data we will be using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0jgq-f-c4lL"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLefT53l-0lJ"
      },
      "source": [
        "## Complete works of Shakespeare\n",
        "http://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "This data set is loaded as a pandas dataframe called `shakespeare_pd`. This dataframe contains a single column called `value` with one row per line of text.\n",
        "\n",
        "## Houston Historical Weather (July 2014-June 2015)\n",
        "https://raw.githubusercontent.com/fivethirtyeight/data/master/us-weather-history/KHOU.csv\n",
        "\n",
        "This data set is loaded as a pandas dataframe called `weather_pd`. The columns are parsed from the csv header.\n",
        "\n",
        "## Houston Electricity Bill Data (July 2014-June 2015)\n",
        "https://data.houstontx.gov/dataset/city-of-houston-electricity-bills/resource/d9ed6d8c-c932-40e4-947c-5931bf83cdc5\n",
        "\n",
        "https://data.houstontx.gov/dataset/1a28386c-92a0-485c-9ad7-b5d8bbd875e6/resource/d9ed6d8c-c932-40e4-947c-5931bf83cdc5/download/8-cohfy2015eebillsreportjuly2014-june2015.xlsx\n",
        "\n",
        "Download the above \".xlsx\" file and save it to the \"Files\" section, under the \"sample_data\" folder. The \"Files\" section is on the left side of the UI, represented by the folder icon.\n",
        "\n",
        "This data set is loaded as a pandas dataframe called `energy_pd`. The first sheet of the Excel file is loaded into the dataframe but there are other sheets in this file that could contain useful data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScdPt2w55B52"
      },
      "source": [
        "shakespeare_pd = pandas.read_csv(\n",
        "    \"https://www.gutenberg.org/cache/epub/100/pg100.txt\",\n",
        "    delimiter=\"/n\",\n",
        "    header=None,\n",
        "    names=[\"value\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3Y4xYPP-9E8"
      },
      "source": [
        "weather_pd = pandas.read_csv(\n",
        "    \"https://raw.githubusercontent.com/fivethirtyeight/data/master/us-weather-history/KHOU.csv\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRlA9M8i75lF"
      },
      "source": [
        "energy_pd = pandas.read_excel(\n",
        "    \"sample_data/8-cohfy2015eebillsreportjuly2014-june2015.xlsx\",\n",
        "    # We will probably use these columns and the default parsing gets\n",
        "    # confused so we force them to strings\n",
        "    dtype = {\n",
        "        \"Voucher Date\": \"str\", \"Due Date\": \"str\"\n",
        "    }\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7RHQoYxSsCI"
      },
      "source": [
        "# [Optional] Install Spark, create the Spark context and convert dataframes\n",
        "If you'd like to use Spark in addition or instead of Pandas run these cells to install Pyspark and convert the Pandas dataframes to Spark dataframes.\n",
        "\n",
        "You can skip these cells if you don't intend on using Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tio8sQB9cJqy"
      },
      "source": [
        "#@title Install Spark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!curl -sL https://dlcdn.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz | tar xz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdC0oyVZcp8O"
      },
      "source": [
        "#@title Create Spark Context\n",
        "import os\n",
        "import findspark\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.5-bin-hadoop3\"\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LQEy1roBLE3"
      },
      "source": [
        "## Convert the Dataframes\n",
        "\n",
        "The dataframes are created as `shakespeare_spark`, `weather_spark`, and `energy_spark`. They are also exposed as temporary views for SparkSQL as `weather`, `energy`, and `shakespeare`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9aum52rBKgW"
      },
      "source": [
        "shakespeare_spark = spark.createDataFrame(shakespeare_pd)\n",
        "weather_spark = spark.createDataFrame(weather_pd)\n",
        "energy_spark = spark.createDataFrame(energy_pd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtjnIS-xKQaX"
      },
      "source": [
        "weather_spark.createOrReplaceTempView(\"weather\")\n",
        "energy_spark.createOrReplaceTempView(\"energy\")\n",
        "shakespeare_spark.createOrReplaceTempView(\"shakespeare\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSdvJDwX-_sK"
      },
      "source": [
        "# Some Examples\n",
        "These cells demonstrate some basic capabilities that might be useful during your interview."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huBY7AQXDgn6"
      },
      "source": [
        "## Printing out dataframe contents\n",
        "Google Colab can render a Pandas dataframe for you. Simply run a cell with the Pandas dataframe on the last line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlufZ4X18PH9"
      },
      "source": [
        "shakespeare_pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0LwbNdgD81G"
      },
      "source": [
        "# Print just two rows\n",
        "weather_pd.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AZ1VMoWEMOP"
      },
      "source": [
        "## Converting Spark Dataframes to Pandas\n",
        "It can be useful to convert a Spark dataframe to Pandas, especially when you want to see the contents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRX7rLRhElP5"
      },
      "source": [
        "# Google Colab won't render a Spark dataframe's contents, as demonstrated here\n",
        "energy_spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjlHtaz25z97"
      },
      "source": [
        "# ... but converting it to Pandas is helpful\n",
        "energy_spark.toPandas().head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYoxom_oGpTS"
      },
      "source": [
        "## Simple Data Selection and Manipulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uS0o7sAGwKx"
      },
      "source": [
        "### Pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3HXI9mVGtTC"
      },
      "source": [
        "#@title Selecting columns\n",
        "weather_pd[['date', 'actual_mean_temp']].head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1V0QMM4DHU0T"
      },
      "source": [
        "#@title Transforming rows / Splitting text\n",
        "shakespeare_pd.applymap(lambda row: row.upper().split()).head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwy5-v90HJgW"
      },
      "source": [
        "### Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSKj_DoPHHwP"
      },
      "source": [
        "#@title Selecting columns\n",
        "weather_spark.select('date', 'actual_mean_temp').limit(5).toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Skg_lCNJIFlx"
      },
      "source": [
        "#@title Exposing a Dataframe as a table\n",
        "shakespeare_spark.createOrReplaceTempView(\"shakespeare_table\")\n",
        "spark.sql(\"SELECT * FROM shakespeare_table ORDER BY value DESC LIMIT 5\").toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZEuk_qjEcih"
      },
      "source": [
        "## Charts and Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuK6viIbFy1X"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sopDeleFIs4"
      },
      "source": [
        "#@title Simple Line Graph\n",
        "x  = weather_pd['date']\n",
        "y1 = weather_pd['actual_max_temp']\n",
        "y2 = weather_pd['actual_min_temp']\n",
        "plt.plot(x, y1, label=\"actual_max_temp\")\n",
        "plt.plot(x, y2, label=\"actual_min_temp\")\n",
        "plt.plot()\n",
        "\n",
        "plt.xlabel(\"x axis\")\n",
        "plt.ylabel(\"y axis\")\n",
        "plt.title(\"Line Graph Example\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6lkVotwGIWA"
      },
      "source": [
        "#@title Histogram\n",
        "plt.hist(weather_pd['actual_max_temp'], bins=20)\n",
        "plt.title(\"Max Temps for a Year\")\n",
        "plt.xlabel(\"Degrees\")\n",
        "plt.ylabel(\"Days\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCZSg4Koqnwv"
      },
      "source": [
        "## Spark SQL\n",
        "It's easy to write SparkSQL and output the results as a Pandas Dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f872uTSaqmeF"
      },
      "source": [
        "spark.sql(\"SELECT * FROM weather LIMIT 5\").toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvZAgcrvqrtK"
      },
      "source": [
        "spark.sql(\"SELECT avg(actual_min_temp),avg(actual_max_temp) FROM weather\").toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci1I2d5PslLM"
      },
      "source": [
        "spark.sql(\"SELECT `kWh usage` FROM energy LIMIT 2\").toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wezh3-G9HjYy"
      },
      "source": [
        "spark.sql(\"SELECT count(actual_min_temp) FROM weather\").toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcH0UTzhHtVd"
      },
      "source": [
        "shakespeare_pd.insert(0, \"ID\", range(len(shakespeare_pd)))\n",
        "shakespeare_pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v15GCSqVR0-Q"
      },
      "source": [
        "###Import TFIDF Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHgPzAELQ5Ye"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "tf_value = tfidf.fit_transform(shakespeare_pd['value'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbdtkQOFSqwH"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "other_tf = tfidf.transform((shakespeare_pd['value']))\n",
        "cos_sim_tfidf = map(lambda x: cosine_similarity(other_tf, x), tf_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxMAG-y4Ukuj"
      },
      "source": [
        "energy_pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCULdrzDYdiF"
      },
      "source": [
        "func = lambda x: x*100\n",
        "energy_pd[['Total Due ($)', 'Franchise Fee ($)']].apply(func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx9c8A3OZOTP"
      },
      "source": [
        "energy_pd[energy_pd['kWh Usage'] < 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions To Solve"
      ],
      "metadata": {
        "id": "cuZi4AkRv1mi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL72vBDBElR3"
      },
      "source": [
        "# Get top 20 words by occurrence in Shakespeare data\n",
        "import requests\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "url = \"https://www.gutenberg.org/cache/epub/100/pg100.txt\"\n",
        "response = requests.get(url)\n",
        "lines = response.text.splitlines()\n",
        "\n",
        "shakespeare_pd =  pd.DataFrame(lines, columns = [\"value\"])\n",
        "\n",
        "# preview\n",
        "# print(shakespeare_pd.head())\n",
        "\n",
        "text = \" \".join(shakespeare_pd[\"value\"])\n",
        "\n",
        "text = text.lower()\n",
        "words = text.split()\n",
        "word_counts = Counter(words)\n",
        "top_20 = word_counts.most_common(20)\n",
        "top_20_df = pd.DataFrame(top_20, columns = [\"word\", \"count\"])\n",
        "print(top_20_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I imported requests because we're using a text file that has a website so it's easier to look at, re (regular expressions) to try and remove any weird punctuation, and the collections and counter so I can count the top 20 words. I then created a url variable so that i could see what the text in the url said and i commented it out, but i viewed the dataframe after seeing only the ID of the line and the value which is one line worth of shakespeare.\n",
        "\n",
        "I made it all into one string so that i wouldn't have to iterate through every single one, and i split the sentences into just words since i'm only curious about the words, and then i made a counter and got the top 20 words."
      ],
      "metadata": {
        "id": "bFePDAUdfMSS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKuKwmxUEuHE"
      },
      "source": [
        "# Given the word count dataframe, get a count of the times these words occur in the Shakespeare Data\n",
        "olde_english_pronouns = [\"thee\", \"thou\", \"thine\"]\n",
        "\n",
        "# from word_counts = Counter(words)\n",
        "for word in olde_english_pronouns:\n",
        "  print(f\"{word}: {word_counts[word]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I just made a for loop that used the old word_counts from problem 1. Counter stored all of the word frequencies including thee, thou and thine, so I just queried it to only look for the olde english pronouns."
      ],
      "metadata": {
        "id": "HOOQYVrqgauY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlBlOza8E1jX"
      },
      "source": [
        "# Create a chart that compares electricity usage to weather data to determine if there are correlations\n",
        "# import matplotlib.pyplot as plt, already imported from earlier\n",
        "import seaborn as sns\n",
        "\n",
        "# for viewing purposes to see column names\n",
        "# print(weather_pd.head())\n",
        "# print(energy_pd.head())\n",
        "\n",
        "energy_pd[\"Voucher Date\"] = pd.to_datetime(energy_pd[\"Voucher Date\"], errors = 'coerce')\n",
        "# print(energy_pd[\"Voucher Date\"].head())\n",
        "weather_pd[\"date\"] = pd.to_datetime(weather_pd[\"date\"], errors = 'coerce')\n",
        "# print(weather_pd[\"date\"].head())\n",
        "\n",
        "merged_df = pd.merge(energy_pd, weather_pd, left_on = \"Voucher Date\", right_on = \"date\")\n",
        "# print(merged_df.head())\n",
        "\n",
        "plt.figure(figsize = (10, 6))\n",
        "sns.scatterplot(data = merged_df, x = \"actual_mean_temp\", y = \"kWh Usage\")\n",
        "plt.xlabel(\"Average Daily Temperature (Fahrenheit)\")\n",
        "plt.ylabel(\"Electricity Usage (kWh)\")\n",
        "plt.title(\"Electricity Usage vs Temperature\")\n",
        "plt.show()\n",
        "\n",
        "correlation = merged_df[\"actual_mean_temp\"].corr(merged_df[\"kWh Usage\"])\n",
        "print(correlation)\n",
        "# Since I got a correlation value of roughly 0.00096 this indicates that there is no linear relationship between\n",
        "# temperature and electricity usage in this dataset. There could be other factors influencing electricity usage\n",
        "# that temperature can't capture alone though"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I first viewed to see the column names and values, that I converted all voucher dates to date time format because pandas can work with that a lot easier. I used errors coerce because i was getting invalid date strings but i needed it to run so i made it run anyway, i did the same thing with weather but for date not voucher date.\n",
        "\n",
        "\n",
        "i then merged the 2 into a singular dataframe and then visualized relationships between electricity usage which was kWh and weather data which was the actual_mean_temp to determine any correlations. I used a scatterplot and saw that there wasn't any relationship, but I used .corr on the merged dataframe just to be sure.\n",
        "\n",
        "Correlation returned a 0.00096, which is proof that there is no linear correlation between just electricity usage and temperature, although there could be some other factors in play.\n",
        "\n"
      ],
      "metadata": {
        "id": "KrZxrbxKld3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Just for checking columns**\n",
        "\n",
        "weather: date, actual mean temp, actual min temp, actual max temp, average min temp, average max temp, record min temp, record max temp, record min temp year, record max temp year, actual precipitation, average precipitation, record precipitation\n",
        "\n",
        "energy: Reliant Contract #, Service Address, ESID, Business Area, Fund, Cost Center, Bill Type, Bill Date, Meter #, Meter Read, Total T&D Charges ($), Current Due ($), Index Charge ($), Total Due ($), Franchise Fee ($), Voucher Date, Billed Demand (KVA), kWh Usage Nodal, Cu Charge (S), Adder Charge (S)"
      ],
      "metadata": {
        "id": "BSgJwnh1kMcx"
      }
    }
  ]
}